---
link: https://www.notion.so/R23-W06-9c9e41f12a884ffeb62a7c0ebd8d3579
notionID: 9c9e41f1-2a88-4ffe-b62a-7c0ebd8d3579
---
### WB Thread 갯수에 따른 성능 차이를 보기 위한 실험

##### #33, 34, 35, 36, 37: Cgroup 설정 삽질과 이를 이용한 WB Thread 수 실험
- 애초에 WB Thread 실험을 하려고 한 이유
	- => ZSSD가 970 EVO대비 빠른 SSD임에도 불구하고 dirty saturation 되었을 때 성능이 대략 1.2GB/s 정도로 두 개가 비슷하게 나옴 => WB thread가 하나라서 성능을 제대로 못 뽑아내는게 아닐까?
- Cgroup을 다르게 설정하면 WB가 그 cgroup마다 하나씩 설정이 됨. 그래서 소스코드 수정 없이 cgroup 설정만으로 위 가정이 진짜인지 보기로 함
- 삽질하다가 cgroup v1은 WB에 차이가 없다는걸 발견. cgroup v2로 세팅
	- cgroup v2로 했었을 때는 WB가 여러개 생성됨을 확인
	- 여러개가 동시에 write req을 생성하고 있고, 실제로 in-flight req 수도 증가함을 확인함

- cgroup 설정한 것과 그렇지 않은 것 사이에 성능차이가 별로 없었음
- 이게 맞나 싶어서 Direct I/O한 결과 rand는 thread수를 증가시켜도 그냥 1GB/s대에 머물러 있고, seq는 thread 수를 증가시키면 direct I/O의 성능이 점차 증가함
	- => 아~ 어차피 rand는 request를 보내는 쓰레드 수를 증가시켜도 별로 성능에 차이가 없고
	- => seq는 request를 보내는 쓰레드 수를 증가시키면 성능에 차이가 나는구나!
- 대충 돌려봤을 때 미세하게 증가하는 듯 하게 보여서
- 3번씩 제대로 돌려서 성능을 평가해봄

- non-cg/cg에서 rand/seq, 4t/8t 의 조합으로 3번씩 돌려서 평균 성능을 얻음 (MiB/s)
	- rand, 4t: 1831/1846
	- rand, 8t: 1141.3/1166
	- seq, 4t: 3574.6/3301.6
	- seq, 8t: 2527.3/2096.6
- 당장의 **결론은 결국 Ext4 파일시스템에서는 writeback thread 수를 늘려도 성능을 늘릴 수는 없다는 것**. 그런데 애초에 실험 설계가 이게 맞나 싶은게 있다
	- rand 4t와 rand 8t 그리고 seq 4t와 seq 8t끼리 비교해보았을 때, non-cg에서는 사실상 같은 스레드 수로 같은 파일들에 대해 같은 패턴으로 돌린 것이기 때문에 서로 성능이 거의 동일하게 나타나야 한다.
	- 그러나 실제로 값을 보면 1831과 1141.3 그리고 3574.6과 2527.3 으로 서로 비교해 보았을 때 엄청나게 큰 폭으로 성능에 차이가 발생한다...?

### 결론
- WB Thread 수 증가로는 성능을 증가시키기는 어렵다 => 새로운 디자인 추가 실패
- 일단은 Setpoint 조절로 트레이드 오프를 가지고 write 성능 증가시키는 방향으로만 디자인 추가하자
- 백그라운드랑 어프로치 작성 시작하자