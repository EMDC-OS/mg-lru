---
link: https://www.notion.so/R23-01-04-f40e849f260240d487309af98dffff48
notionID: f40e849f-2602-40d4-8730-9af98dffff48
---
### NOT 성능 변화 경향성 차이

**ftrace 결과**
#6
- FIO 워밍업 제외 3분 중 30초만 tracing했을 때 do_try_to_free_pages가
	- NOT의 성능이 고꾸라지는 random에서는 2956 번 호출된다.
	- 성능이 잘 나오는 normal에서는 171 번 호출된다.
	- => 17배

#7
- do_try_to_pages가 저렇게 큰폭으로 차이나는 이유가 무엇인가?
	- 첫 째로 의심되는 것은 dirty page수의 차이로 인해서, kswapd가 백그라운드에서 reclaim을 엄청 느리게 하고 있고, 그래서 write하는 애가 메모리가 부족해서 직접 비우는 수가 많아진다.
		- 근데 이때 직접 비우는 작업 조차도 dirty가 많기 때문에 read보다 느려진다?
		
	- Van Read(1), Van Write(2), Van-not write(3) random 성능 확인
		- shrink_node: 363,524/277,588/759,373
		- shrink_lruvec: 5,732/3,657/11,789
		- shrink_active_list: 4,999/0/6,690
		- shrink_inactive_list: 6,905/3,303/9,340
		- shrink_page_lsit: 3,448/3,139/1,896
		
	- Van Read(1), Van Write(2), Van-not write(3) normal 성능 확인
		- shrink_node: 433,718/244,176/791,761
		- shrink_lruvec: 6,891/3,518/11,999
		- shrink_active_list: 5,114/14/32/
		- shrink_inactive_list: 6,288/1,526/12,479
		- shrink_page_lsit: 3,461/1,445/10,940
		
	=> Reclaim 전체 과정으로 볼 때 NOT가 오래 걸리는 것은 맞지만 그건 random과 normal 마찬가지이다. 심지어 read가 기존 write보다 더 오래 걸리는데 그것 대비 쓰루풋은 엄청 빠르다.
	=> shrink_page_list 시간의 평균으로만 보면 dirty를 직접 쫓아내는시간이 길어져서 reclaim시간이 길어진 것은 아니다. 뭔가 다른 이유가 있다.
	=> 왜이리 shrink_active_list의 시간에 차이가 크게크게 나는가? 막상 vmstat으로 보면 random과 normal의 active file page 수 차이가 엄청 크게 나는 것도 아니다. random: 약 6만, normal: 약 4만
	=> kswapd 에서와 direct 에서의 reclaim 시간 차이를 한 번 보자!
	
	1. dirty page 수 확인해보자. 
	2. writeback은 어떤 방식으로 이루어지고 있느건지도 확인해보자
	3. Write trottling은 어떤 방식?

**조금 더 근본적으로 왜 Read에서는 필요하지 않았던 I/O Throttling이라는 녀석이 write에서는 필요한 것이냐**
- Dirty라는 애가 둘 사이에 명확하게 존재하는 차이점이다.
- kswapd에서 메모리를 비워주는 시간에 차이가 발생?
	- ㅇㅋ 같은 양을 비우는데 있어서 dirty는 write까지 해야하니까 그럴 듯해 실제로 그래?
- 그런데 


### Desktop에서 FLRU와 Finer-LRU가 성능 증가가 없는 이유

- 이를 확인하기 위해서는 어떤 방법을 취해야 할까?
	- 일단, Tail latency의 절대값들을 보고 기존처럼 나빠지고 있는 것이 맞는지 확인한뒤에, 그게 맞다면 eBPF 통해서 기존 FLRU가 해결했던 문제점이 나타나고 있는지를 확인한다.