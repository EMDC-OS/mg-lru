---
link: https://www.notion.so/R23-02-24-26d11b36cf99453b9ef8f84dfda7aac6
notionID: 26d11b36-cf99-453b-9ef8-f84dfda7aac6
---
#48
##### I/O Throttling 구현
- 1차 구현 완료
	- 구현 중 변경사항
		1. 감소시킬 때 어제 생각한대로 nr_dirty/nr_scanned 퍼센트만큼의 비율로 줄여버리면 너무 과하게 줄이는 경향이 생길거라 예측. 따라서, nr_dirty/nr_scanned x 10 퍼센트에 해당하는 만큼을 비율이 아닌 빼기로 해서 줄여버리는 것으로 변경, vm_dirty_ratio를 한 번에 최대 10%를 줄이게 됨
		2. fluctuation을 아예 빼버리려고 하다가, 우선은 증가시킬 때와 감소시킬 때 모두 5% 이상 변할 때만 실제로 변경시키는 것으로 수정 
	- concurrency 관련
		- 변수 하나에 대한 변경이기에 atomic variable로 하는 것이 제일 깔끔하다. 그러나 해당 변수가 proc file system을 통한 수정과 엮여 있는데, 여기에 atomic variable을 어떻게 적용해야하는지 파악하지 못하였다. 따라서 우선은 변경으로 인한 성능 저하가 좀 있더라도 spinlock을 통해 concurrency를 제어하도록 수정
- 나중에 cgroup 고려해야할 수도?
	- 어떤식으로? 
	- => 지금방식으로 계산 후에, 페이지수 혹은 메모리 크기 비율로 weight를 주어서 반영이 좀 작아지게 만든다.

- 1차 구현에 대한 동작 테스트 결과
	- qemu에서의 테스트를 포기하고 데스크탑의 램디스크에 세팅해서 실험한 결과 동작은 하고 있음.
	- 그러나 vm_dirty_ratio가 20 -> 100 -> 20 을 계속 반복함
	- 왜? 크게 3가지 측면에서 잘못되었다.
		1. 실험 환경(유추): 지금 보면 throttling하려고 시작하는 시점부터 이미 dirty page수가 available 페이지 수에 근접해버린다. => 아마 실험 환경을 만들 때 전체 메모리를 너무 작게 잡아서 그런 것으로 추정
		2. 그렇게 해서 dirty가 높아져버린 순간부터는 vm_dirty_ratio가 감소하더라도 다시금 바로 그 높아진 값으로 바로 회기할 가능성이 거의 100에 수렴한다. => 값을 낮춘다고 해서 그 dirty가 곧바로 낮아지는 것이 아닐 것이기 때문에 거의 비슷한 dirty수를 유지할 것이고, 그렇기 때문에 다음번 Throttling에서 바로 해당 dirty를 freerun으로 만족시키는 vm_dirty_ratio값으로, 즉 이전 값으로 회기할 것이다.
		3. 줄이는 것도 dirty페이지를 맞닥뜨린 순간부터는 계속 큰 배치로 마주할 확률이 높기 때문에 지금 설정으로는 너무 큼지막하게 작아져서 곧바로 20에 수렴하게 된다.