diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 80bd08e..0abdcd3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -318,7 +318,6 @@ struct lruvec {
 	struct list_head		lists[NR_LRU_LISTS];
 	/* per lruvec lru_lock for memcg */
 	spinlock_t			lru_lock;
-	spinlock_t			cost_lock;
 	/*
 	 * These track the cost of reclaiming one LRU - file or anon -
 	 * over the other. As the observed cost of reclaiming one LRU
@@ -328,6 +327,7 @@ struct lruvec {
 	unsigned long			file_cost;
 	/* Non-resident age, driven by LRU movement */
 	atomic_long_t			nonresident_age;
+	atomic_long_t			activate_age;
 	/* Refaults at the time of last reclaim cycle */
 	unsigned long			refaults[ANON_AND_FILE];
 	/* Various lruvec state flags (enum lruvec_flags) */
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 0c0fed1..06e769d 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -353,6 +353,7 @@ static inline swp_entry_t folio_swap_entry(struct folio *folio)
 
 /* linux/mm/workingset.c */
 void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages);
+void workingset_age_activate(struct lruvec *lruvec, unsigned long nr_pages);
 void *workingset_eviction(struct folio *folio, struct mem_cgroup *target_memcg);
 void workingset_refault(struct folio *folio, void *shadow);
 void workingset_activation(struct folio *folio);
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 2ebef1d..147204d 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -88,7 +88,6 @@ static int vm_highmem_is_dirtyable;
  * The generator of dirty data starts writeback at this percentage
  */
 static int vm_dirty_ratio = 20;
-static int adaptive_cache = 1;
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
@@ -1556,14 +1555,6 @@ static inline void wb_dirty_limits(struct dirty_throttle_control *dtc)
  * perform some writeout.
  */
 static unsigned long prev_nres_age = 0;
-static atomic64_t last_tht_time = ATOMIC_INIT(0);
-static unsigned long cuml_dirty_pages = 0;
-static unsigned long prev_size_ratio = 200;
-static unsigned long prev_cache_count = 0;
-static unsigned long cache_bounded_count = 0;
-static unsigned long prev_cache_type = 0;
-static unsigned long prev_cache_total = 0;
-static bool prev_free = false;
 static void balance_dirty_pages(struct bdi_writeback *wb,
 				unsigned long pages_dirtied)
 {
@@ -1586,27 +1577,15 @@ static void balance_dirty_pages(struct bdi_writeback *wb,
 	bool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;
 	unsigned long start_time = jiffies;
 
-	struct pglist_data *pgdat;
-	struct lruvec *lruvec;
+  struct pglist_data *pgdat;
+  struct lruvec *lruvec;
+  unsigned long nres_age = 0;
+  unsigned long active_age = 0;
+  unsigned long pnres_age;
+  unsigned long nr_active_file;
+  unsigned long nr_inactive_file;
 
-	unsigned long nres_age = 0;
-	unsigned long pnres_age;
-
-	unsigned long new_ratio = 0;
-	unsigned long size_ratio = 0;
-	unsigned long thresh_ratio = 0;
-	unsigned long eviction_ratio = 0;
-	unsigned long nr_active_file;
-	unsigned long nr_inactive_file;
-	unsigned long nr_active_anon;
-	unsigned long nr_inactive_anon;
-	unsigned long nr_free;
-	unsigned long last_tht = atomic64_read(&last_tht_time);
-
-
-	cuml_dirty_pages += pages_dirtied;
-	
-  for (;;) {
+	for (;;) {
 		unsigned long now = jiffies;
 		unsigned long dirty, thresh, bg_thresh;
 		unsigned long m_dirty = 0;	/* stop bogus uninit warnings */
@@ -1676,23 +1655,6 @@ static void balance_dirty_pages(struct bdi_writeback *wb,
 			unsigned long m_intv;
 
 free_running:
-      prev_nres_age = 0;
-			prev_size_ratio = 200;
-			prev_cache_type = 0;
-			prev_cache_count = 0;
-			prev_cache_total = 0;
-			cache_bounded_count = 0;
-			cuml_dirty_pages = 0;
-			if (time_after(now, last_tht + msecs_to_jiffies(1000))) {
-				if (last_tht != atomic64_cmpxchg(&last_tht_time, last_tht, now))
-					goto out1;
-				//last_tht_time = now;
-				new_ratio = vm_dirty_ratio * 80 / 100;
-				vm_dirty_ratio = new_ratio > 20 ? new_ratio : 20;
-				prev_free = true;
-			}
-out1:
-
 			intv = dirty_poll_interval(dirty, thresh);
 			m_intv = ULONG_MAX;
 
@@ -1794,102 +1756,25 @@ out1:
 		 * future periods by updating the virtual time; otherwise just
 		 * do a reset, as it may be a light dirtier.
 		 */
-    if (prev_free || time_after(now, last_tht + msecs_to_jiffies(1000))) {
-      //if (timeout) {
-      if (last_tht != atomic64_cmpxchg(&last_tht_time, last_tht, now))
-        goto out2;
-      //last_tht_time = now;
-
-      nr_active_file = global_node_page_state(NR_ACTIVE_FILE);
-      nr_inactive_file = global_node_page_state(NR_INACTIVE_FILE);
-      nr_active_anon = global_node_page_state(NR_ACTIVE_ANON);
-      nr_inactive_anon = global_node_page_state(NR_INACTIVE_ANON);
-      nr_free = global_zone_page_state(NR_FREE_PAGES);
-
-      size_ratio = ((nr_active_file + nr_inactive_file + nr_free) * 100)
-        / (nr_active_file + nr_inactive_file
-            + nr_active_anon + nr_inactive_anon + nr_free + 1);
-
-      thresh_ratio = size_ratio > 40 ? (size_ratio-20) : 20;
-
-      new_ratio = vm_dirty_ratio * 120 / 100;
-      if (new_ratio > size_ratio)
-        new_ratio = size_ratio;
-
-      for_each_online_pgdat(pgdat) {
-        lruvec = mem_cgroup_lruvec(NULL, pgdat);
-        nres_age += atomic_long_read(&lruvec->nonresident_age);
-      }
-
-      pnres_age = prev_nres_age;
-      prev_nres_age = nres_age;
-      eviction_ratio = (nres_age-pnres_age) * 100 / cuml_dirty_pages;
-
-      if (!adaptive_cache || pnres_age == 0 || eviction_ratio == 0)
-        goto just_size;	
-
-      if (abs(prev_size_ratio - size_ratio) > 5) {
-        prev_cache_type = 0;
-        prev_cache_count = 0;
-        prev_cache_total = 0;
-        cache_bounded_count = 0;
-      } else {
-        if (eviction_ratio > 100) {
-          if (prev_cache_type == 2) {
-            prev_cache_count = 0;
-            cache_bounded_count = 0;
-            prev_cache_total = 0;
-          }
-
-          prev_cache_type = 1;
-          prev_cache_total += eviction_ratio - 100;
-          prev_cache_count++;
-        }
-        else if (eviction_ratio < 50) {
-          if (prev_cache_type == 1) {
-            prev_cache_count = 0;
-            cache_bounded_count = 0;
-            prev_cache_total = 0;
-          }
-
-          prev_cache_type = 2;
-          prev_cache_total += 50 - eviction_ratio;
-          prev_cache_count++;
-        } else {
-          cache_bounded_count++;
-          if (cache_bounded_count == 10) {
-            //	prev_cache_count++;
-            cache_bounded_count = 0;
-          }
-        }
-
-        if (prev_cache_count >= 5) {
-          if (size_ratio > (prev_cache_total/prev_cache_count))
-            new_ratio = size_ratio - (prev_cache_total/prev_cache_count);
-          else
-            new_ratio = 0;
-        }
-      }
-
-just_size:
-      prev_size_ratio = size_ratio;
-      vm_dirty_ratio = new_ratio > thresh_ratio ? new_ratio : thresh_ratio;
-
-      trace_throttling_change_value(eviction_ratio, cuml_dirty_pages,
-          size_ratio, new_ratio, prev_cache_total, prev_cache_count);
-
-      cuml_dirty_pages = 0;
-      prev_free = false;
+    for_each_online_pgdat(pgdat) {
+	    lruvec = mem_cgroup_lruvec(NULL, pgdat);
+      nres_age += atomic_long_read(&lruvec->nonresident_age);
+      active_age += atomic_long_read(&lruvec->activate_age);
     }
-out2:
+    pnres_age = prev_nres_age;
+    prev_nres_age = nres_age;
+	  nr_active_file = global_node_page_state(NR_ACTIVE_FILE);
+	  nr_inactive_file = global_node_page_state(NR_INACTIVE_FILE);
 
 		if (pause < min_pause) {
+    trace_throttling_change_value(nres_age, pnres_age, 
+        nr_active_file, nr_inactive_file, active_age, 0);
 			trace_balance_dirty_pages(wb,
 						  sdtc->thresh,
 						  sdtc->bg_thresh,
 						  sdtc->dirty,
-              sdtc->wb_thresh,
-              sdtc->wb_dirty,
+						  sdtc->wb_thresh,
+						  sdtc->wb_dirty,
 						  dirty_ratelimit,
 						  task_ratelimit,
 						  pages_dirtied,
@@ -1913,6 +1798,8 @@ out2:
 		}
 
 pause:
+    trace_throttling_change_value(nres_age, pnres_age, 
+        nr_active_file, nr_inactive_file, active_age, 0);
 		trace_balance_dirty_pages(wb,
 					  sdtc->thresh,
 					  sdtc->bg_thresh,
@@ -2222,15 +2109,6 @@ static int page_writeback_cpu_online(unsigned int cpu)
 static const unsigned long dirty_bytes_min = 2 * PAGE_SIZE;
 
 static struct ctl_table vm_page_writeback_sysctls[] = {
-  {
-		.procname   = "adaptive_cache",
-		.data       = &adaptive_cache,
-		.maxlen     = sizeof(adaptive_cache),
-		.mode       = 0644,
-		.proc_handler   = dirty_background_ratio_handler,
-		.extra1     = SYSCTL_ZERO,
-		.extra2     = SYSCTL_ONE_HUNDRED,
-	},
 	{
 		.procname   = "dirty_background_ratio",
 		.data       = &dirty_background_ratio,
diff --git a/mm/swap.c b/mm/swap.c
index 7b74b04..034bb24 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -273,8 +273,7 @@ void lru_note_cost(struct lruvec *lruvec, bool file, unsigned int nr_pages)
 		 *    rcu lock, so would be safe even if the page was on the LRU
 		 *    and could move simultaneously to a new lruvec).
 		 */
-		spin_lock_irq(&lruvec->cost_lock);
-		//spin_lock_irq(&lruvec->lru_lock);
+		spin_lock_irq(&lruvec->lru_lock);
 		/* Record cost event */
 		if (file)
 			lruvec->file_cost += nr_pages;
@@ -298,8 +297,7 @@ void lru_note_cost(struct lruvec *lruvec, bool file, unsigned int nr_pages)
 			lruvec->file_cost /= 2;
 			lruvec->anon_cost /= 2;
 		}
-		spin_unlock_irq(&lruvec->cost_lock);
-		//spin_unlock_irq(&lruvec->lru_lock);
+		spin_unlock_irq(&lruvec->lru_lock);
 	} while ((lruvec = parent_lruvec(lruvec)));
 }
 
diff --git a/mm/vmscan.c b/mm/vmscan.c
index a912af2..691aaf6 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2311,8 +2311,10 @@ static unsigned int move_pages_to_lru(struct lruvec *lruvec,
 		add_page_to_lru_list(page, lruvec);
 		nr_pages = thp_nr_pages(page);
 		nr_moved += nr_pages;
-		if (PageActive(page))
+		if (PageActive(page)) {
 			workingset_age_nonresident(lruvec, nr_pages);
+			workingset_age_activate(lruvec, nr_pages);
+    }
 	}
 
 	/*
@@ -2918,10 +2920,6 @@ static void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)
 	 */
 	scan_adjusted = (!cgroup_reclaim(sc) && !current_is_kswapd() &&
 			 sc->priority == DEF_PRIORITY);
-	if (!current_is_kswapd()) {
-		nr[LRU_ACTIVE_FILE] = 0;
-		nr[LRU_ACTIVE_ANON] = 0;
-	}
 
 	blk_start_plug(&plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
diff --git a/mm/workingset.c b/mm/workingset.c
index ff97dd7..ca12386 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -243,6 +243,12 @@ void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)
 		atomic_long_add(nr_pages, &lruvec->nonresident_age);
 	} while ((lruvec = parent_lruvec(lruvec)));
 }
+void workingset_age_activate(struct lruvec *lruvec, unsigned long nr_pages)
+{
+	do {
+		atomic_long_add(nr_pages, &lruvec->activate_age);
+	} while ((lruvec = parent_lruvec(lruvec)));
+}
 
 /**
  * workingset_eviction - note the eviction of a folio from memory
@@ -381,6 +387,7 @@ void workingset_refault(struct folio *folio, void *shadow)
 
 	folio_set_active(folio);
 	workingset_age_nonresident(lruvec, nr);
+	workingset_age_activate(lruvec, nr);
 	mod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);
 
 	/* Folio was active prior to eviction */
@@ -412,8 +419,9 @@ void workingset_activation(struct folio *folio)
 	 */
 	memcg = folio_memcg_rcu(folio);
 	if (!mem_cgroup_disabled() && !memcg)
-    goto out;
-  workingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));
+		goto out;
+	workingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));
+	workingset_age_activate(folio_lruvec(folio), folio_nr_pages(folio));
 out:
 	rcu_read_unlock();
 }
